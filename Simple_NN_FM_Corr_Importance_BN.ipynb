{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIxVr7mkkAgl"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/Multiome_Cell/data/multimodal-single-cell-as-sparse-matrix.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q keras_tuner\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import keras_tuner\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "from sklearn.model_selection import GroupKFold,KFold\n",
        "import lightgbm as lgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import scipy\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from tqdm.notebook import tqdm\n",
        "import joblib"
      ],
      "metadata": {
        "id": "FngUmRKDkEni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed(seed=0):\n",
        "  np.random.seed(seed)  # Numpy module.\n",
        "  random.seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed) \n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "\n",
        "seed(19990829)"
      ],
      "metadata": {
        "id": "Kg4s8DDKg1C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs = scipy.sparse.load_npz('/content/train_cite_inputs_values.sparse.npz')\n",
        "train_inputs = np.array(train_inputs.todense())\n",
        "test_inputs = scipy.sparse.load_npz('/content/test_cite_inputs_values.sparse.npz')\n",
        "test_inputs = np.array(test_inputs.todense())\n",
        "train_targets = scipy.sparse.load_npz('/content/train_cite_targets_values.sparse.npz')\n",
        "train_targets = np.array(train_targets.todense())\n",
        "cols = np.load('/content/train_cite_inputs_idxcol.npz',allow_pickle=True)['columns']\n",
        "target_cols = np.load('/content/train_cite_targets_idxcol.npz',allow_pickle=True)['columns']"
      ],
      "metadata": {
        "id": "Y6gw1K25g2de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 分析特征中数值为0的占比\n",
        "analysis_1_train = (train_inputs == 0).sum(axis=0) / len(train_inputs)\n",
        "analysis_1_test = (test_inputs == 0).sum(axis=0) / len(test_inputs)"
      ],
      "metadata": {
        "id": "8wMU8nBwWI_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(analysis_1_train)"
      ],
      "metadata": {
        "id": "cI25pXebWVLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(analysis_1_test)"
      ],
      "metadata": {
        "id": "c7XQ_EacW0vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "not_constant_cols = list(set([k for k,v in enumerate(train_inputs.std(axis=0) == 0) if v == False] + [k for k,v in enumerate(test_inputs.std(axis=0) == 0) if v == False]))\n",
        "not_constant_cols_2 = list(set([k for k,v in enumerate(analysis_1_train >= 0.999) if v == False] + [k for k,v in enumerate(analysis_1_test >= 0.999) if v == False]))\n",
        "# not_constant_cols = not_constant_cols_2\n",
        "\n",
        "# important_cols = []\n",
        "# for y_col in target_cols:\n",
        "#     important_cols += [i for i in range(len(cols)) if y_col in cols[i]]\n",
        "# not_constant_cols = list(set([k for k,v in enumerate(train_inputs.std(axis=0) == 0) if v == False] + [k for k,v in enumerate(test_inputs.std(axis=0) == 0) if v == False]))\n",
        "important_cols_tmp = [\n",
        " 'ENSG00000135218_CD36',\n",
        " 'ENSG00000010278_CD9',\n",
        " 'ENSG00000204287_HLA-DRA',\n",
        " 'ENSG00000117091_CD48',\n",
        " 'ENSG00000004468_CD38',\n",
        " 'ENSG00000173762_CD7',\n",
        " 'ENSG00000137101_CD72',\n",
        " 'ENSG00000019582_CD74',\n",
        " 'ENSG00000169442_CD52',\n",
        " 'ENSG00000170458_CD14',\n",
        " 'ENSG00000272398_CD24',\n",
        " 'ENSG00000026508_CD44',\n",
        " 'ENSG00000114013_CD86',\n",
        " 'ENSG00000174059_CD34',\n",
        " 'ENSG00000139193_CD27',\n",
        " 'ENSG00000105383_CD33',\n",
        " 'ENSG00000085117_CD82',\n",
        " 'ENSG00000177455_CD19',\n",
        " 'ENSG00000002586_CD99',\n",
        " 'ENSG00000196126_HLA-DRB1',\n",
        " 'ENSG00000135404_CD63',\n",
        " 'ENSG00000012124_CD22',\n",
        " 'ENSG00000134061_CD180',\n",
        " 'ENSG00000105369_CD79A',\n",
        " 'ENSG00000116824_CD2',\n",
        " 'ENSG00000010610_CD4',\n",
        " 'ENSG00000139187_KLRG1',\n",
        " 'ENSG00000204592_HLA-E',\n",
        " 'ENSG00000090470_PDCD7',\n",
        " 'ENSG00000206531_CD200R1L',\n",
        "'ENSG00000166710_B2M',\n",
        " 'ENSG00000198034_RPS4X',\n",
        " 'ENSG00000188404_SELL',\n",
        " 'ENSG00000130303_BST2',\n",
        " 'ENSG00000128040_SPINK2',\n",
        " 'ENSG00000206503_HLA-A',\n",
        " 'ENSG00000108107_RPL28',\n",
        " 'ENSG00000143226_FCGR2A',\n",
        " 'ENSG00000133112_TPT1',\n",
        " 'ENSG00000166091_CMTM5',\n",
        " 'ENSG00000026025_VIM',\n",
        " 'ENSG00000205542_TMSB4X',\n",
        " 'ENSG00000109099_PMP22',\n",
        " 'ENSG00000145425_RPS3A',\n",
        " 'ENSG00000172247_C1QTNF4',\n",
        " 'ENSG00000072274_TFRC',\n",
        " 'ENSG00000234745_HLA-B',\n",
        " 'ENSG00000075340_ADD2',\n",
        " 'ENSG00000119865_CNRIP1',\n",
        " 'ENSG00000198938_MT-CO3',\n",
        " 'ENSG00000135046_ANXA1',\n",
        " 'ENSG00000235169_SMIM1',\n",
        " 'ENSG00000101200_AVP',\n",
        " 'ENSG00000167996_FTH1',\n",
        " 'ENSG00000163565_IFI16',\n",
        " 'ENSG00000117450_PRDX1',\n",
        " 'ENSG00000124570_SERPINB6',\n",
        " 'ENSG00000112077_RHAG',\n",
        " 'ENSG00000051523_CYBA',\n",
        " 'ENSG00000107130_NCS1',\n",
        " 'ENSG00000055118_KCNH2',\n",
        " 'ENSG00000029534_ANK1',\n",
        " 'ENSG00000169567_HINT1',\n",
        " 'ENSG00000142089_IFITM3',\n",
        " 'ENSG00000139278_GLIPR1',\n",
        " 'ENSG00000142227_EMP3',\n",
        " 'ENSG00000076662_ICAM3',\n",
        " 'ENSG00000143627_PKLR',\n",
        " 'ENSG00000130755_GMFG',\n",
        " 'ENSG00000160593_JAML',\n",
        " 'ENSG00000095932_SMIM24',\n",
        " 'ENSG00000197956_S100A6',\n",
        " 'ENSG00000171476_HOPX',\n",
        " 'ENSG00000116675_DNAJC6',\n",
        " 'ENSG00000100448_CTSG',\n",
        " 'ENSG00000100368_CSF2RB',\n",
        " 'ENSG00000047648_ARHGAP6',\n",
        " 'ENSG00000198918_RPL39',\n",
        " 'ENSG00000196154_S100A4',\n",
        " 'ENSG00000233968_AL157895.1',\n",
        " 'ENSG00000137642_SORL1',\n",
        " 'ENSG00000133816_MICAL2',\n",
        " 'ENSG00000130208_APOC1',\n",
        " 'ENSG00000105610_KLF1'\n",
        "]\n",
        "\n",
        "\n",
        "important_cols = [i for i in range(len(cols)) if cols[i] in important_cols_tmp]\n",
        "\n",
        "print(len(not_constant_cols))\n",
        "print(len(important_cols))"
      ],
      "metadata": {
        "id": "rf_Ea4xIjN6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_importance = joblib.load('/content/drive/MyDrive/Multiome_Cell/data/result/corr_importance.pkl')\n",
        "corr_importance = pd.DataFrame(corr_importance)\n",
        "corr_importance = corr_importance.T\n",
        "corr_important_feature = corr_importance[corr_importance > 0.5].sum(axis=1)[corr_importance[corr_importance > 0.5].sum(axis=1) !=0 ].index"
      ],
      "metadata": {
        "id": "knqRqm_q16wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_important_feature"
      ],
      "metadata": {
        "id": "0j9kuEDi2VZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "important_cols = list(set(important_cols + list(corr_important_feature)))"
      ],
      "metadata": {
        "id": "lQ0uXapf2JyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_not_constant = train_inputs[:,not_constant_cols]\n",
        "train_important = train_inputs[:,important_cols]\n",
        "test_not_constant = test_inputs[:,not_constant_cols]\n",
        "test_important = test_inputs[:,important_cols]\n",
        "train_test = np.concatenate([train_not_constant,test_not_constant],axis=0)\n",
        "train_test_important = np.concatenate([train_important,test_important],axis=0)"
      ],
      "metadata": {
        "id": "HGzbDrAlobdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "train_test_important = StandardScaler().fit_transform(train_test_important)\n",
        "pca_cite = TruncatedSVD(n_components=64, random_state=42)\n",
        "inputs_pca = pca_cite.fit_transform(train_test)\n",
        "train_inputs = np.concatenate([inputs_pca[:len(train_inputs)],train_test_important[:len(train_inputs)]],axis=1)\n",
        "test_inputs = np.concatenate([inputs_pca[len(train_inputs):],train_test_important[len(train_inputs):]],axis=1)"
      ],
      "metadata": {
        "id": "29k2YUdgm23m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs.shape"
      ],
      "metadata": {
        "id": "kUdGogcfcNkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_important.shape[1]"
      ],
      "metadata": {
        "id": "flaVjIR028iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_targets  -= train_targets.mean(axis=1).reshape(-1,1)\n",
        "train_targets /= train_targets.std(axis=1).reshape(-1,1)"
      ],
      "metadata": {
        "id": "JodWv4vIoAm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits = 9\n",
        "meta_df = pd.read_parquet('/content/metadata.parquet')\n",
        "meta_df = meta_df[meta_df.technology == 'citeseq']\n",
        "\n",
        "conditions = [\n",
        "    meta_df['donor'].eq(27678) & meta_df['day'].eq(2),\n",
        "    meta_df['donor'].eq(27678) & meta_df['day'].eq(3),\n",
        "    meta_df['donor'].eq(27678) & meta_df['day'].eq(4),\n",
        "    meta_df['donor'].eq(27678) & meta_df['day'].eq(7),\n",
        "    meta_df['donor'].eq(13176) & meta_df['day'].eq(2),\n",
        "    meta_df['donor'].eq(13176) & meta_df['day'].eq(3),\n",
        "    meta_df['donor'].eq(13176) & meta_df['day'].eq(4),\n",
        "    meta_df['donor'].eq(13176) & meta_df['day'].eq(7),\n",
        "    meta_df['donor'].eq(31800) & meta_df['day'].eq(2),\n",
        "    meta_df['donor'].eq(31800) & meta_df['day'].eq(3),\n",
        "    meta_df['donor'].eq(31800) & meta_df['day'].eq(4),\n",
        "    meta_df['donor'].eq(31800) & meta_df['day'].eq(7),\n",
        "    meta_df['donor'].eq(32606) & meta_df['day'].eq(2),\n",
        "    meta_df['donor'].eq(32606) & meta_df['day'].eq(3),\n",
        "    meta_df['donor'].eq(32606) & meta_df['day'].eq(4),\n",
        "    meta_df['donor'].eq(32606) & meta_df['day'].eq(7)\n",
        "    ]\n",
        "\n",
        "# create a list of the values we want to assign for each condition\n",
        "values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
        "\n",
        "# create a new column and use np.select to assign values to it using our lists as arguments\n",
        "meta_df['group'] = np.select(conditions, values)"
      ],
      "metadata": {
        "id": "2kLxOr9S1lEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cite_index_dict = {}\n",
        "count = 0\n",
        "for i in tqdm(np.load('/content/train_cite_inputs_idxcol.npz',allow_pickle=True)['index']):\n",
        "  cite_index_dict[i] = count\n",
        "  count += 1\n",
        "meta_df['train_test_index'] = meta_df['cell_id'].map(cite_index_dict)\n",
        "meta_df['train_test_index'] = meta_df['cell_id'].map(cite_index_dict)\n",
        "meta_df_group = meta_df[~meta_df['train_test_index'].isna()]\n",
        "meta_df_group.index = meta_df_group['train_test_index']\n",
        "kf = GroupKFold(n_splits=n_splits)\n",
        "for index,(train_index , test_index) in enumerate(kf.split(meta_df_group,meta_df_group,meta_df_group['group'])):\n",
        "  print(meta_df_group.loc[train_index,'group'].unique(),meta_df_group.loc[test_index,'group'].unique())"
      ],
      "metadata": {
        "id": "vOLJ3WyY2ao2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_score(y_true, y_pred):\n",
        "    \"\"\"Scores the predictions according to the competition rules. \n",
        "    \n",
        "    It is assumed that the predictions are not constant.\n",
        "    \n",
        "    Returns the average of each sample's Pearson correlation coefficient\"\"\"\n",
        "    if type(y_true) == pd.DataFrame: y_true = y_true.values\n",
        "    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n",
        "    corrsum = 0\n",
        "    for i in range(len(y_true)):\n",
        "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
        "    return corrsum / len(y_true)\n",
        "\n",
        "def negative_correlation_loss(y_true, y_pred):\n",
        "    \"\"\"Negative correlation loss function for Keras\n",
        "\n",
        "    Precondition:\n",
        "    y_true.mean(axis=1) == 0\n",
        "    y_true.std(axis=1) == 1\n",
        "    \n",
        "    Returns:\n",
        "    -1 = perfect positive correlation\n",
        "    1 = totally negative correlation\n",
        "    \"\"\"\n",
        "    my = K.mean(tf.convert_to_tensor(y_pred), axis=1)\n",
        "    my = tf.tile(tf.expand_dims(my, axis=1), (1, y_true.shape[1]))\n",
        "    ym = y_pred - my\n",
        "    r_num = K.sum(tf.multiply(y_true, ym), axis=1)\n",
        "    r_den = tf.sqrt(K.sum(K.square(ym), axis=1) * float(y_true.shape[-1]))\n",
        "    r = tf.reduce_mean(r_num / r_den)\n",
        "    return - r"
      ],
      "metadata": {
        "id": "Zu3lYC7bOLor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_START = 0.01\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "\n",
        "\n",
        "def my_model(hp, n_inputs = train_inputs.shape[1]):\n",
        "    \"\"\"Sequential neural network\n",
        "    \n",
        "    Returns a compiled instance of tensorflow.keras.models.Model.\n",
        "    \"\"\"\n",
        "    activation = 'swish'\n",
        "\n",
        "\n",
        "    reg1 = hp.Float(\"reg1\", min_value=1e-8, max_value=1e-4, sampling=\"log\")\n",
        "    reg2 = hp.Float(\"reg2\", min_value=1e-10, max_value=1e-5, sampling=\"log\")\n",
        "\n",
        "    inputs = Input(shape=(n_inputs, ))\n",
        "\n",
        "    # attention_probs = Dense(n_inputs, activation='softmax', name='attention_vec')(inputs)\n",
        "    # attention_mul =  tf.multiply(inputs, attention_probs)\n",
        "\n",
        "    x0 = tf.keras.layers.BatchNormalization()(inputs)\n",
        "    x0 = tf.keras.layers.Dropout(0.1)(x0)\n",
        "    x0 = Dense(256, kernel_regularizer=tf.keras.regularizers.l2(reg1),\n",
        "              activation = activation,\n",
        "             )(x0)\n",
        "\n",
        "    x1 = tf.keras.layers.BatchNormalization()(x0)\n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x1)\n",
        "    x1 = Dense(256, kernel_regularizer=tf.keras.regularizers.l2(reg1),\n",
        "              activation = activation,\n",
        "             )(x1)\n",
        "\n",
        "    x2 = tf.keras.layers.BatchNormalization()(x1)\n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x2)\n",
        "    x2 = Dense(256, kernel_regularizer=tf.keras.regularizers.l2(reg1),\n",
        "              activation = activation,\n",
        "             )(x2)\n",
        "\n",
        "    x3 = tf.keras.layers.BatchNormalization()(x2)\n",
        "    x3 = tf.keras.layers.Dropout(0.1)(x3)\n",
        "    x3 = Dense(128, kernel_regularizer=tf.keras.regularizers.l2(reg1),\n",
        "              activation = activation,\n",
        "             )(x3)\n",
        "    \n",
        "    \n",
        "    inputs_important = inputs[:,-train_important.shape[1]:]\n",
        "    v = tf.random_uniform_initializer(minval=-0.001, maxval=0.001, seed=19990829)((inputs_important.shape[1],300))\n",
        "    inter_part1 = K.pow(tf.matmul(inputs_important, v), 2)\n",
        "    inter_part2 = tf.matmul(K.pow(inputs_important, 2), K.pow(v, 2))\n",
        "    pair_interactions = tf.subtract(inter_part1, inter_part2)\n",
        "    \n",
        "     \n",
        "\n",
        "    x = Concatenate()([x0,x1,x2,x3,pair_interactions])\n",
        "\n",
        "    # x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = Dense(140, kernel_regularizer=tf.keras.regularizers.l2(reg2),\n",
        "             )(x)\n",
        "\n",
        "    \n",
        "    regressor = Model(inputs, x)\n",
        "    regressor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR_START),\n",
        "                      metrics=['mse',negative_correlation_loss],\n",
        "                      loss=negative_correlation_loss\n",
        "                     )\n",
        "    \n",
        "    return regressor"
      ],
      "metadata": {
        "id": "AWYTFrOA88od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_hp = keras_tuner.HyperParameters()\n",
        "best_hp.values = {\n",
        "                'reg1': 6.89e-6, #8e-6,\n",
        "                'reg2': 0, #2e-6,\n",
        "                'units1': 256,\n",
        "                'units2': 256,\n",
        "                'units3': 256,\n",
        "                'units4': 128\n",
        "                } \n",
        "# 'reg1': 6.89e-6, #8e-6,"
      ],
      "metadata": {
        "id": "Lq_XhMLyObby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "VERBOSE = 1 # set to 2 for more output\n",
        "EPOCHS = 1000\n",
        "N_SPLITS = 9\n",
        "\n",
        "\n",
        "kf = GroupKFold(n_splits = N_SPLITS)\n",
        "score_list = []\n",
        "va_pred = []\n",
        "for fold,(train_index , test_index) in enumerate(kf.split(meta_df_group,meta_df_group,meta_df_group['group'])):\n",
        "    model = None\n",
        "    gc.collect()\n",
        "\n",
        "    X_tr , y_tr = train_inputs[train_index] , train_targets[train_index] \n",
        "    X_va ,  y_va  = train_inputs[test_index] , train_targets[test_index]\n",
        "    \n",
        "\n",
        "    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, \n",
        "                           patience=4, verbose=VERBOSE)\n",
        "    es = EarlyStopping(monitor=\"val_loss\",\n",
        "                       patience=12, \n",
        "                       verbose=0,\n",
        "                       mode=\"min\", \n",
        "                       restore_best_weights=True)\n",
        "    callbacks = [lr, es, tf.keras.callbacks.TerminateOnNaN()]\n",
        "\n",
        "    # Construct and compile the model\n",
        "    model = my_model(best_hp, X_tr.shape[1])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_tr, y_tr, \n",
        "                        validation_data=(X_va, y_va), \n",
        "                        epochs=EPOCHS,\n",
        "                        verbose=VERBOSE,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        shuffle=True,\n",
        "                        callbacks=callbacks)\n",
        "    del X_tr, y_tr\n",
        "    \n",
        "    model.save(f\"/content/drive/MyDrive/Multiome_Cell/data/model/tensorflow_dnn/tf_cite_fm_model_v2_{fold}\")\n",
        "    history = history.history\n",
        "    callbacks, lr = None, None\n",
        "    \n",
        "    # We validate the model\n",
        "    y_va_pred = model.predict(X_va, batch_size=len(X_va))\n",
        "    va_pred.append(y_va_pred)\n",
        "    corrscore = correlation_score(y_va, y_va_pred)\n",
        "\n",
        "    print(f\"Fold {fold}: {es.stopped_epoch:3} epochs, corr =  {corrscore:.5f}\")\n",
        "    del es, X_va#, y_va, y_va_pred\n",
        "    score_list.append(corrscore)\n",
        "\n",
        "# Show overall score\n",
        "print(f\"corr = {np.array(score_list).mean():.5f}\")"
      ],
      "metadata": {
        "id": "F8IwiHDbgHtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(score_list , f\"corr = {np.array(score_list).mean():.5f}\")"
      ],
      "metadata": {
        "id": "KbhC1pBA076t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict():\n",
        "  preds = np.zeros((len(test_inputs), 140), dtype=np.float32)\n",
        "  for fold in range(N_SPLITS):\n",
        "      print(f\"Predicting with fold {fold}\")\n",
        "      model = load_model(f\"/content/drive/MyDrive/Multiome_Cell/data/model/tensorflow_dnn/tf_cite_fm_model_v2_{fold}\",custom_objects={'negative_correlation_loss': negative_correlation_loss})\n",
        "      preds += model.predict(test_inputs)\n",
        "\n",
        "  test_pred = preds / N_SPLITS\n",
        "\n",
        "  submission = pd.read_csv('/content/drive/MyDrive/Multiome_Cell/data/result/multi_dnn_model_v1_cite.csv',\n",
        "                            index_col='row_id', squeeze=True)\n",
        "\n",
        "\n",
        "  submission.iloc[:len(test_pred.ravel())] = test_pred.ravel()\n",
        "  assert not submission.isna().any()\n",
        "  submission.to_csv('/content/drive/MyDrive/Multiome_Cell/data/result/tf_cite_model_submission_v2.csv')\n",
        "  display(submission)"
      ],
      "metadata": {
        "id": "CeGtyvTTObg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict()"
      ],
      "metadata": {
        "id": "OsB1s7881tTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hYhu_Dlg2tH-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}